{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install keplergl\n!pip3 install pickle5\n!pip install pygeohash\n!pip install lightgbm\n!pip install h3\n!cp -r /kaggle/input/cian-parser-v1/cianparser-main/* \n!pip install -e /kaggle/input/cian-parser-v1/cianparser-main\n!jupyter nbextension install --py --sys-prefix keplergl # can be skipped for notebook 5.3 and above\n!jupyter nbextension enable --py --sys-prefix keplergl # can be skipped for notebook 5.3 and above","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:23:23.659000Z","iopub.execute_input":"2022-05-29T12:23:23.659808Z","iopub.status.idle":"2022-05-29T12:25:47.891000Z","shell.execute_reply.started":"2022-05-29T12:23:23.659683Z","shell.execute_reply":"2022-05-29T12:25:47.889533Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import sys\nimport pandas as pd\nimport os\nfrom keplergl import KeplerGl\nimport numpy as np\n\nimport pickle5 as pickle\nfrom h3 import h3\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KDTree\nimport math\nfrom sklearn.model_selection import KFold\n\nfrom  sklearn.metrics import mean_absolute_error\n\nimport pygeohash as pgh\nimport seaborn as sns\n\ndef load_pickle(file_path):\n    with open(file_path, 'rb') as handle:\n        return pickle.load(handle)\n        \ndef save_pickle(obj, filepath): \n    with open(filepath, 'wb') as handle:\n        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\npd.set_option('display.max_columns', None)","metadata":{"ExecuteTime":{"end_time":"2022-05-25T15:17:07.109692Z","start_time":"2022-05-25T15:17:06.017931Z"},"execution":{"iopub.status.busy":"2022-05-29T12:25:47.893162Z","iopub.execute_input":"2022-05-29T12:25:47.893556Z","iopub.status.idle":"2022-05-29T12:25:50.433221Z","shell.execute_reply.started":"2022-05-29T12:25:47.893516Z","shell.execute_reply":"2022-05-29T12:25:50.431934Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"column_mapper = {\n    'Автозапчасти для иномарок': 'autoparts',\n    'Авторемонт и техобслуживание (СТО)': 'autoremont',\n    'Алкогольные напитки': 'alcohols',\n    'Аптеки': 'pharmacies',\n    'Банки': 'banks',\n    'Быстрое питание': 'fastfood',\n    'Доставка готовых блюд': 'delivery',\n    'Женская одежда': 'female_clothes',\n    'Кафе': 'cafe',\n    'Косметика / Парфюмерия': 'cosmetics',\n    'Ногтевые студии': 'nails',\n    'Овощи / Фрукты': 'vegetables',\n    'Парикмахерские': 'hairs',\n    'Платёжные терминалы': 'pay_terminals',\n    'Постаматы': 'mails',\n    'Продуктовые магазины': 'products',\n    'Пункты выдачи интернет-заказов': 'internet_orders',\n    'Рестораны': 'restaurants',\n    'Страхование': 'insurance',\n    'Супермаркеты': 'supermarkets',\n    'Цветы': 'flowers',\n    'Шиномонтаж': 'tires'\n}\ncat_features = [] \nnum_features = ['population'] + list(column_mapper.values())\ntarget = 'target'\nfeatures = cat_features + num_features","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:26:49.772077Z","iopub.execute_input":"2022-05-29T12:26:49.772493Z","iopub.status.idle":"2022-05-29T12:26:49.780372Z","shell.execute_reply.started":"2022-05-29T12:26:49.772456Z","shell.execute_reply":"2022-05-29T12:26:49.779607Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def population_metric(hexs, top_n=20):\n    \n    best_hexs = set([hex_id for hex_id in hexs if hex_id in uncovered][:top_n])\n    small_tree = KDTree(df_isochrones[df_isochrones[geo_id].apply(lambda id: id in best_hexs)][['lon', 'lat']].values)\n    df_uncovered = df_population[df_population[geo_id].apply(lambda id: id in uncovered)]\n    \n    dist, ind = small_tree.query(df_uncovered[['lon', 'lat']].values, k=1)\n    new_peoples = df_uncovered[dist < mean_radius]['population'].sum()\n    uplift = (new_peoples) / (total_peoples) * 100\n    \n    return uplift","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:26:52.140567Z","iopub.execute_input":"2022-05-29T12:26:52.141462Z","iopub.status.idle":"2022-05-29T12:26:52.150884Z","shell.execute_reply.started":"2022-05-29T12:26:52.141413Z","shell.execute_reply":"2022-05-29T12:26:52.149841Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\ndef drop_dups(df):\n    return df.drop_duplicates(subset=[geo_id])\n\nROOT = '/kaggle/input/geo-branch-data/train/train'\ngeo_id = 'geo_h3_10'\ndf_population = drop_dups(pd.read_csv(f\"{ROOT}/rosstat_population_all_cities.csv\"))\ndf_isochrones = drop_dups(pd.read_csv(f\"{ROOT}/isochrones_walk_dataset.csv\"))\ndf_companies = drop_dups(pd.read_csv(f\"{ROOT}/osm_amenity.csv\"))\ndf_target = drop_dups(pd.read_csv('/kaggle/input/bankmachinesrussia/target_hakaton_spb.csv', sep=';', encoding='Windows-1251'))\n\ngeo_id_mapper = dict(df_isochrones.apply(lambda row: (row[geo_id], (row['lat'], row['lon'])), axis=1).tolist())\n\n    \ndef prepare_df(df):\n    _df = df.merge(df_population, on=geo_id, suffixes=(None, '_y'), how='left').drop(['lat', 'lon', 'city'], axis=1)\n    _df = _df.merge(df_companies, on=geo_id, suffixes=(None, '_y'), how='left').drop(['city', 'lat', 'lon'], axis=1).fillna(0)\n\n    _df = _df.rename(columns=column_mapper)\n    for feature in num_features:\n        _df[feature] = (_df[feature] - _df[feature].mean())/ _df[feature].std()\n\n    for feature in cat_features:\n        _df[f'{feature}_ohe'] = _df[feature]\n    _df = pd.get_dummies(_df, columns=[f'{col}_ohe' for col in cat_features], prefix=cat_features)\n\n    return _df\n\ndef prepare_df2(df):\n    _df = df.merge(df_companies, on=geo_id, suffixes=(None, '_y'), how='left').drop(['city', 'lat', 'lon'], axis=1).fillna(0)\n\n    _df = _df.rename(columns=column_mapper)\n    for feature in num_features:\n        _df[feature] = (_df[feature] - _df[feature].mean())/ _df[feature].std()\n\n    for feature in cat_features:\n        _df[f'{feature}_ohe'] = _df[feature]\n    _df = pd.get_dummies(_df, columns=[f'{col}_ohe' for col in cat_features], prefix=cat_features)\n\n    ohe_cols = [col for col in _df.columns if col.startswith(tuple([item + '_' for item in cat_features]))]\n    X = _df[num_features + ohe_cols].values\n    \n    return X\n\ndef prepare_target(_df):\n    _df[target] = _df[target] / _df['atm_cnt']\n    _df[target] = (_df[target] - _df[target].mean())/ _df[target].std()\n\n    _df = _df[_df[target] - _df[target].mean() < 3 * _df[target].std()]\n    return _df\n\ndef get_regression(df_target):\n\n    df_target = prepare_df(df_target)\n    df_target = prepare_target(df_target)\n\n    ohe_cols = [col for col in df_target.columns if col.startswith(tuple([item + '_' for item in cat_features]))]\n    X = df_target[num_features + ohe_cols].values\n    y = df_target[target].values\n    y = (y - y.min()) / (y.max() - y.min())\n    \n    k_fold = KFold(5)\n    scores = []\n    lr = linear_model.LassoCV(max_iter=1000)\n    for k, (train, test) in enumerate(k_fold.split(X, y)):\n        lr.fit(X[train], y[train])\n        scores.append(mean_absolute_error(y[test], lr.predict(X[test])))\n    print(f'Mean linear regression MAE: {float(np.mean(scores))}')\n    \n    return lr\nlr = get_regression(df_target)\n# df_isochrones = df_isochrones.merge(df_population, on=geo_id, how='left', suffixes=[None, '_y']).drop(['lat_y', 'lon_y', 'city_y'], axis=1)\n# pop_values = df_population['population'].tolist()\n\n# pop_tree = KDTree(df_population[['lon', 'lat']].values)\n# def replace_na(row, tree):\n#     if not math.isnan(row['population']):\n#         return row\n#     lon = float(row['lon'])\n#     lat = float(row['lat'])\n#     dist, ind = tree.query([[lon, lat]], k=3)\n#     row['population'] = float(np.mean([pop_values[i] for i in ind[0]]))\n#     return row\n    \n# df_isochrones = df_isochrones.apply(lambda row: replace_na(row, pop_tree), axis=1)\n\ndef get_mean_radius(df_isochrones):\n    def get_radius(row):\n        lat = float(row['lat'])\n        lon = float(row['lon'])\n        poly_str = row['walk_15min'].replace('POLYGON ((', '').replace('))', '')\n        points = poly_str.split(',')\n        dist = 0\n        for point_str in points:\n            items = point_str.strip().split(' ')\n            _lon = float(items[0])\n            _lat = float(items[1])\n            dist += np.sqrt((lat - _lat) ** 2 + (lon - _lon) ** 2)\n        return float(dist) / len(points)\n\n    mean_radius = (df_isochrones.apply(get_radius, axis=1)).mean() # 5 mins\n    \n    return mean_radius\n\nmean_radius = get_mean_radius(df_isochrones)\nprint(f'Mean isochrone radius: {mean_radius}')\n\ncities = df_isochrones['city'].drop_duplicates().tolist()\nprint('Available cities: ', cities)\n\nfilled_hexs = set(df_target[geo_id].drop_duplicates().tolist())\nfilled_trees = {}\nuncovered = {}\ncovered = {}\ntotal_peoples = {}\nuncovered_df = {}\nuncovered_trees = {} \n\nfor city in cities:\n    city_isochrones = df_isochrones[df_isochrones['city'].apply(lambda val: val == city)]\n    filled_trees[city] = KDTree(city_isochrones[city_isochrones[geo_id].apply(lambda id: id in filled_hexs)][['lon', 'lat']].values)\n    dist, ind = filled_trees[city].query(city_isochrones[['lon', 'lat']].values, k=1)\n    uncovered[city] = set(city_isochrones[dist > mean_radius][geo_id].drop_duplicates().tolist())\n    covered[city] = set(city_isochrones[dist < mean_radius][geo_id].drop_duplicates().tolist())\n    peoples = df_population[df_population['city'].apply(lambda val: val == city)]['population'].sum()\n    total_peoples[city] = df_population[df_population['city'].apply(lambda val: val == city) & df_population[geo_id].apply(lambda id: id in covered[city])]['population'].sum()\n    print(f'City - {city}, peoples  - {peoples}, peoples covered - {total_peoples[city]}')\n    \n    df = df_population[df_population['city'].apply(lambda val: val == city) & df_population[geo_id].apply(lambda id: id in uncovered[city])]\n    df['score'] = lr.predict(prepare_df2(df))\n    uncovered_df[city] = df\n    uncovered_trees[city] = KDTree(uncovered_df[city][['lon', 'lat']].values)\n    \n\ndef get_predictions(city, uncovered_df, total_peoples, uncovered_trees, mean_radius, geo_id='geo_h3_10', n=20):\n    uncovered_pops = uncovered_df[city]['population'].tolist()\n    uncovered_hexs = uncovered_df[city][geo_id].tolist()\n    uncovered_scores = uncovered_df[city]['score'].tolist()\n\n    indices = uncovered_trees[city].query_radius(uncovered_df[city][['lon', 'lat']].values, r=mean_radius)\n\n    tmp_covered = set()\n    selected = []\n    total_lift = 0\n    lifts = []\n    for current_terminal in range(n):\n        best_lift = 0\n        best_item = 0\n        for item_id, items in enumerate(indices):\n            added = sum([uncovered_pops[item] for item in set(items) if item not in tmp_covered])\n            lift = added / total_peoples[city] * 100\n            score = uncovered_scores[item_id]\n            if lift * score > best_lift:\n                best_lift = lift * score\n                best_item = item_id\n\n        total_lift += best_lift\n        lifts.append(total_lift)\n        selected.append(uncovered_hexs[best_item])\n        tmp_covered.update(set(indices[best_item]))\n\n    return lifts, selected\n\ntotal_lift, selected = get_predictions(cities[1], uncovered_df, total_peoples, uncovered_trees, mean_radius, geo_id, 5)\ncoords = [geo_id_mapper[id] for id in selected]\nprint(f'Total lift: {total_lift[-1]}')\nprint('Hexagon indices: ', selected)\nprint(coords)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:26:55.236743Z","iopub.execute_input":"2022-05-29T12:26:55.237137Z","iopub.status.idle":"2022-05-29T12:27:08.578199Z","shell.execute_reply.started":"2022-05-29T12:26:55.237106Z","shell.execute_reply":"2022-05-29T12:27:08.576856Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\ncity_id = 1\nlifts, selected = get_predictions(cities[city_id], uncovered_df, total_peoples, uncovered_trees, mean_radius, geo_id, 30)\nprint(len(lifts))\nprint(cities[city_id])\nsns.lineplot(x=np.arange(1, 31), y=lifts)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:27:10.554529Z","iopub.execute_input":"2022-05-29T12:27:10.554971Z","iopub.status.idle":"2022-05-29T12:27:12.808717Z","shell.execute_reply.started":"2022-05-29T12:27:10.554936Z","shell.execute_reply":"2022-05-29T12:27:12.807760Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"lifts[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:27:12.810565Z","iopub.execute_input":"2022-05-29T12:27:12.811066Z","iopub.status.idle":"2022-05-29T12:27:12.818244Z","shell.execute_reply.started":"2022-05-29T12:27:12.811018Z","shell.execute_reply":"2022-05-29T12:27:12.817312Z"},"trusted":true},"execution_count":10,"outputs":[]}]}